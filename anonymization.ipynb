{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "data = pd.read_csv(\"generation\\\\database.csv\")\n",
    "\n",
    "numerical_qis = ['age']\n",
    "categorical_qis = ['gender', 'city', 'education', 'profession']\n",
    "all_qis = categorical_qis + numerical_qis\n",
    "explicit_identifier = ['person_id', 'first_name', 'last_name']\n",
    "sensitive_data = ['annual_income']\n",
    "statistic = \"range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSON data from file\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop specified columns from a DataFrame\n",
    "def drop_EI(df, EI):\n",
    "    return df.drop(columns=EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_parents(node, target, path=None, key=None):\n",
    "    \"\"\"Trova tutti i genitori di un nodo target in un albero JSON\"\"\"\n",
    "    if path is None:\n",
    "        path = []\n",
    "\n",
    "    current_name = node.get('name', None) if isinstance(node, dict) else None\n",
    "\n",
    "    if isinstance(node, dict):\n",
    "        # Caso in cui il nodo è un dizionario\n",
    "        if key and node.get(key) == target:\n",
    "            # Se è stato trovato il nodo target all'interno del dizionario\n",
    "            return path + [current_name]\n",
    "\n",
    "        for k, v in node.items():\n",
    "            if isinstance(v, list) and target in v:\n",
    "                # Se il nodo target è presente nella lista\n",
    "                return path + [current_name]\n",
    "            elif isinstance(v, (dict, list)):\n",
    "                # Ricorsivamente cerca nei sotto-nodi\n",
    "                result = find_parents(v, target, path + [current_name] if current_name else path, key)\n",
    "                if result:\n",
    "                    return result\n",
    "    elif isinstance(node, list):\n",
    "        # Caso in cui il nodo è una lista\n",
    "        for item in node:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                # Ricorsivamente cerca nei sotto-nodi\n",
    "                result = find_parents(item, target, path, key)\n",
    "                if result:\n",
    "                    return result\n",
    "            elif item == target:\n",
    "                # Se il nodo target è stato trovato nella lista\n",
    "                return path + [current_name]\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_target_parents_in_json(file_path, target, key=None):\n",
    "    \"\"\"Trova tutti i genitori di un nodo target in un file JSON specificato. \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return find_parents(data, target, key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_common_ancestor(file_path, target1, target2, key=None):\n",
    "    \"\"\"Trova l'antenato comune più basso di due nodi target in un file JSON. \"\"\"\n",
    "    parents1 = find_target_parents_in_json(file_path, target1, key)\n",
    "    parents2 = find_target_parents_in_json(file_path, target2, key)\n",
    "\n",
    "    if not parents1 or not parents2:\n",
    "        return None\n",
    "\n",
    "    # Rimuove i valori None e deduplica le liste di genitori\n",
    "    parents1 = [parent for parent in parents1 if parent]\n",
    "    parents2 = [parent for parent in parents2 if parent]\n",
    "\n",
    "    # Trova l'antenato comune più basso confrontando i percorsi\n",
    "    lca = None\n",
    "    for p1, p2 in zip(parents1, parents2):\n",
    "        if p1 == p2:\n",
    "            lca = p1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(dataframe, column, k):\n",
    "    \"\"\"Splits the dataframe along a certain column respecting k value\"\"\"\n",
    "    if column in dataframe.select_dtypes(include='number').columns:\n",
    "        median_value = dataframe[column].median()\n",
    "        left_partition = dataframe[dataframe[column] <= median_value]\n",
    "        right_partition = dataframe[dataframe[column] > median_value]\n",
    "    elif column in dataframe.select_dtypes(include='object').columns:\n",
    "        sorted_values = sorted(dataframe[column].unique())\n",
    "        middle_index = len(sorted_values) // 2\n",
    "        middle_value = sorted_values[middle_index]\n",
    "        left_partition = dataframe[dataframe[column] <= middle_value]\n",
    "        right_partition = dataframe[dataframe[column] > middle_value]\n",
    "    else:\n",
    "        # Skip columns that are neither numeric nor categorical\n",
    "        return None, None\n",
    "    \n",
    "    if len(left_partition) >= k and len(right_partition) >= k:\n",
    "        return left_partition, right_partition\n",
    "    else:\n",
    "        # Adjust partitions if lengths are less than k\n",
    "        left_partition = dataframe.iloc[:k]\n",
    "        right_partition = dataframe.iloc[k:]\n",
    "        return left_partition, right_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left partition shape for column 'gender': (3, 9)\n",
      "Right partition shape for column 'gender': (997, 9)\n",
      "Left partition shape for column 'city': (544, 9)\n",
      "Right partition shape for column 'city': (456, 9)\n",
      "Left partition shape for column 'education': (737, 9)\n",
      "Right partition shape for column 'education': (263, 9)\n",
      "Left partition shape for column 'profession': (528, 9)\n",
      "Right partition shape for column 'profession': (472, 9)\n",
      "Left partition shape for column 'age': (510, 9)\n",
      "Right partition shape for column 'age': (490, 9)\n"
     ]
    }
   ],
   "source": [
    "# Esegui lo split per ogni colonna in all_qis\n",
    "for column in all_qis:\n",
    "    left, right = splitter(data, column, k)\n",
    "    if left is not None and right is not None:\n",
    "        print(f\"Left partition shape for column '{column}': {left.shape}\")\n",
    "        print(f\"Right partition shape for column '{column}': {right.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_partition(dataset, k, sensitive_data):\n",
    "    \"\"\"Splits the dataset in partitions recursively.\"\"\"\n",
    "    \n",
    "    def axe_to_split(dataframe, sensitive_data):\n",
    "        # Find column with highest cardinality (unique values) to split on\n",
    "        columns_to_exclude = [col for col in dataframe.columns if col in sensitive_data]\n",
    "        max_cardinality_column = dataframe.drop(columns_to_exclude, axis=1).nunique().idxmax()\n",
    "        return max_cardinality_column\n",
    "    \n",
    "    # Base case: if dataset size is smaller than k*2, add it to partitions list\n",
    "    if len(dataset) < k * 2:\n",
    "        dataframe_partitions.append(dataset)\n",
    "    else:\n",
    "        # Split according to column with highest cardinality\n",
    "        axe = axe_to_split(dataset, sensitive_data)\n",
    "        left_partition, right_partition = splitter(dataset, axe, k)\n",
    "        \n",
    "        # Recursively partition left and right partitions\n",
    "        recursive_partition(left_partition, k, sensitive_data)\n",
    "        recursive_partition(right_partition, k, sensitive_data)\n",
    "\n",
    "# Example usage:\n",
    "dataframe_partitions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_partition(partition, qis, json_files, statistic):\n",
    "    \"\"\"Generalizza una partizione del dataset sostituendo i valori dei quasi-identificatori con la loro generalizzazione.\n",
    "        Se il quasi-identificatore è numerico, viene generalizzato con il range di valori o la media.\n",
    "        Se il quasi-identificatore è categorico, viene generalizzato con l'antenato comune più basso (LCA). \"\"\"\n",
    "    numerical_qis = [qi for qi in qis if partition[qi].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    for qi in qis:\n",
    "        # Ordina la partizione in base al quasi-identificatore corrente\n",
    "        partition = partition.sort_values(by=qi)\n",
    "        \n",
    "        # Controlla se tutti i valori sono uguali per il quasi-identificatore corrente\n",
    "        if partition[qi].iloc[0] != partition[qi].iloc[-1]:\n",
    "            if qi in numerical_qis:\n",
    "                if statistic == 'range':\n",
    "                    # Se il quasi-identificatore è numerico, generalizza con il range di valori\n",
    "                    min_val = partition[qi].iloc[0]\n",
    "                    max_val = partition[qi].iloc[-1]\n",
    "                    s = f\"[{min_val}-{max_val}]\"\n",
    "                elif statistic == 'mean':\n",
    "                    # Se il quasi-identificatore è numerico e la statistica è 'mean', generalizza con la media\n",
    "                    mean_val = partition[qi].mean()\n",
    "                    s = f\"[{mean_val}]\"\n",
    "                else:\n",
    "                    raise ValueError(\"Statistic must be 'range' or 'mean'\")\n",
    "            else:\n",
    "                # Se il quasi-identificatore è categorico, cerca l'antenato comune più basso (LCA)\n",
    "                unique_values = sorted(set(partition[qi]))\n",
    "                if len(unique_values) == 1:\n",
    "                    lca = unique_values[0]\n",
    "                else:\n",
    "                    lca = unique_values[0]\n",
    "                    for value in unique_values[1:]:\n",
    "                        lca_candidate = find_lowest_common_ancestor(json_files[qi], lca, value, key='name')\n",
    "                        if lca_candidate:\n",
    "                            lca = lca_candidate\n",
    "                        else:\n",
    "                            lca = 'ANY'\n",
    "                            break\n",
    "                        \n",
    "                s = f\"[{lca}]\"\n",
    "            \n",
    "            # Sostituisce i valori del quasi-identificatore con la generalizzazione trovata\n",
    "            partition[qi] = [s] * partition[qi].size\n",
    "    \n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = {\n",
    "    'city': 'generation/cities.json',\n",
    "    'profession': 'generation/jobs.json',\n",
    "    'education': 'generation/educations.json',\n",
    "    'gender': 'generation/genders.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mondrian(database, k, qis, sd, ei, json_files):\n",
    "    global dataframe_partitions\n",
    "    dataframe_partitions = []\n",
    "\n",
    "    # Drop explicit identifiers from the database\n",
    "    database = drop_EI(database, ei)\n",
    "    \n",
    "    # Perform recursive partitioning\n",
    "    recursive_partition(database, k, sd)\n",
    "\n",
    "    # Generalize partitions\n",
    "    generalized_partitions = []\n",
    "    for i, partition in enumerate(dataframe_partitions):\n",
    "        generalized_partition = generalize_partition(partition, qis, json_files, statistic='range')\n",
    "        generalized_partitions.append(generalized_partition)\n",
    "    \n",
    "    # Concatenate generalized partitions into anonymized_data\n",
    "    anonymized_data = pd.concat(generalized_partitions, ignore_index=True)\n",
    "    \n",
    "    # Save anonymized data to CSV\n",
    "    anonymized_data.to_csv('anonymized.csv', index=False)\n",
    "    print(\"Dati anonimizzati salvati in anonymized.csv\")\n",
    "    \n",
    "    # Print debugging information\n",
    "    print(f\"Numero di partizioni: {len(dataframe_partitions)}\")\n",
    "    total_rows = sum(len(partition) for partition in dataframe_partitions)\n",
    "    print(f\"Numero totale di righe nelle partizioni: {total_rows}\")\n",
    "    print(f\"Numero originale di righe: {len(database)}\")\n",
    "    print(f\"Numero totale di righe anonimizzate: {len(anonymized_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati anonimizzati salvati in anonymized.csv\n",
      "Numero di partizioni: 275\n",
      "Numero totale di righe nelle partizioni: 1000\n",
      "Numero originale di righe: 1000\n",
      "Numero totale di righe anonimizzate: 1000\n"
     ]
    }
   ],
   "source": [
    "anonymized_data = mondrian(data, k, all_qis, sensitive_data, explicit_identifier, json_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
