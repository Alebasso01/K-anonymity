{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "data = pd.read_csv(\"generation\\\\database.csv\")\n",
    "\n",
    "numerical_qis = ['age']\n",
    "categorical_qis = ['gender', 'city', 'education', 'profession']\n",
    "all_qis = categorical_qis + numerical_qis\n",
    "explicit_identifier = ['person_id', 'first_name', 'last_name']\n",
    "sensitive_data = ['annual_income']\n",
    "statistic = \"range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSON data from file\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop specified columns from a DataFrame\n",
    "def drop_EI(df, EI):\n",
    "    return df.drop(columns=EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_parents(node, target, path=None, key=None):\n",
    "    \"\"\"Trova tutti i genitori di un nodo target in un albero JSON.\n",
    "\n",
    "    Args:\n",
    "        node (dict or list): Nodo dell'albero JSON.\n",
    "        target (str): Nodo target da cercare.\n",
    "        path (list, optional): Percorso attuale nell'albero JSON. Default: None.\n",
    "        key (str, optional): Chiave per cercare il nodo target all'interno di un dizionario. Default: None.\n",
    "\n",
    "    Returns:\n",
    "        list or None: Lista di nomi dei genitori del nodo target oppure None se non trovato.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        path = []\n",
    "\n",
    "    current_name = node.get('name', None) if isinstance(node, dict) else None\n",
    "\n",
    "    if isinstance(node, dict):\n",
    "        # Caso in cui il nodo è un dizionario\n",
    "        if key and node.get(key) == target:\n",
    "            # Se è stato trovato il nodo target all'interno del dizionario\n",
    "            return path + [current_name]\n",
    "\n",
    "        for k, v in node.items():\n",
    "            if isinstance(v, list) and target in v:\n",
    "                # Se il nodo target è presente nella lista\n",
    "                return path + [current_name]\n",
    "            elif isinstance(v, (dict, list)):\n",
    "                # Ricorsivamente cerca nei sotto-nodi\n",
    "                result = find_parents(v, target, path + [current_name] if current_name else path, key)\n",
    "                if result:\n",
    "                    return result\n",
    "    elif isinstance(node, list):\n",
    "        # Caso in cui il nodo è una lista\n",
    "        for item in node:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                # Ricorsivamente cerca nei sotto-nodi\n",
    "                result = find_parents(item, target, path, key)\n",
    "                if result:\n",
    "                    return result\n",
    "            elif item == target:\n",
    "                # Se il nodo target è stato trovato nella lista\n",
    "                return path + [current_name]\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_target_parents_in_json(file_path, target, key=None):\n",
    "    \"\"\"Trova tutti i genitori di un nodo target in un file JSON specificato.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Percorso del file JSON.\n",
    "        target (str): Nodo target da cercare.\n",
    "        key (str, optional): Chiave per cercare il nodo target all'interno di un dizionario. Default: None.\n",
    "\n",
    "    Returns:\n",
    "        list or None: Lista di nomi dei genitori del nodo target oppure None se non trovato.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return find_parents(data, target, key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'antenato comune più basso di 'Bridge Engineer' e 'Highway Engineer' è: Structural Engineer\n",
      "L'antenato comune più basso di 'Master's Degree' e 'Doctoral Degree' è: Graduate School\n",
      "L'antenato comune più basso di 'Boston' e 'Chicago' è: USA\n",
      "L'antenato comune più basso di 'Male' e 'Female' è: ANY-GENDER\n"
     ]
    }
   ],
   "source": [
    "def find_lowest_common_ancestor(file_path, target1, target2, key=None):\n",
    "    \"\"\"Trova l'antenato comune più basso di due nodi target in un file JSON.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Percorso del file JSON.\n",
    "        target1 (str): Primo nodo target.\n",
    "        target2 (str): Secondo nodo target.\n",
    "        key (str, optional): Chiave per cercare il nodo target all'interno di un dizionario. Default: None.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Nome dell'antenato comune più basso oppure None se non trovato.\n",
    "    \"\"\"\n",
    "    parents1 = find_target_parents_in_json(file_path, target1, key)\n",
    "    parents2 = find_target_parents_in_json(file_path, target2, key)\n",
    "\n",
    "    if not parents1 or not parents2:\n",
    "        return None\n",
    "\n",
    "    # Rimuove i valori None e deduplica le liste di genitori\n",
    "    parents1 = [parent for parent in parents1 if parent]\n",
    "    parents2 = [parent for parent in parents2 if parent]\n",
    "\n",
    "    # Trova l'antenato comune più basso confrontando i percorsi\n",
    "    lca = None\n",
    "    for p1, p2 in zip(parents1, parents2):\n",
    "        if p1 == p2:\n",
    "            lca = p1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return lca\n",
    "\n",
    "# Esempi di utilizzo\n",
    "\n",
    "# jobs.json\n",
    "file_path_jobs = 'generation/jobs.json'\n",
    "target1_jobs = 'Bridge Engineer'\n",
    "target2_jobs = 'Highway Engineer'\n",
    "lca_jobs = find_lowest_common_ancestor(file_path_jobs, target1_jobs, target2_jobs)\n",
    "print(f\"L'antenato comune più basso di '{target1_jobs}' e '{target2_jobs}' è: {lca_jobs}\")\n",
    "\n",
    "# educations.json\n",
    "file_path_educations = 'generation/educations.json'\n",
    "target1_educations = \"Master's Degree\"\n",
    "target2_educations = \"Doctoral Degree\"\n",
    "lca_educations = find_lowest_common_ancestor(file_path_educations, target1_educations, target2_educations, key='name')\n",
    "print(f\"L'antenato comune più basso di '{target1_educations}' e '{target2_educations}' è: {lca_educations}\")\n",
    "\n",
    "# cities.json\n",
    "file_path_cities = 'generation/cities.json'\n",
    "target1_cities = 'Boston'\n",
    "target2_cities = 'Chicago'\n",
    "lca_cities = find_lowest_common_ancestor(file_path_cities, target1_cities, target2_cities)\n",
    "print(f\"L'antenato comune più basso di '{target1_cities}' e '{target2_cities}' è: {lca_cities}\")\n",
    "\n",
    "# gender.json\n",
    "file_path_gender = 'generation/genders.json'\n",
    "target1_gender = 'Male'\n",
    "target2_gender = 'Female'\n",
    "lca_gender = find_lowest_common_ancestor(file_path_gender, target1_gender, target2_gender, key='name')\n",
    "print(f\"L'antenato comune più basso di '{target1_gender}' e '{target2_gender}' è: {lca_gender}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(dataframe, column, k):\n",
    "    \"\"\"Splits the dataframe along a certain column respecting k value\"\"\"\n",
    "    if column in dataframe.select_dtypes(include='number').columns:\n",
    "        median_value = dataframe[column].median()\n",
    "        left_partition = dataframe[dataframe[column] <= median_value]\n",
    "        right_partition = dataframe[dataframe[column] > median_value]\n",
    "    elif column in dataframe.select_dtypes(include='object').columns:\n",
    "        unique_values = dataframe[column].unique()\n",
    "        middle_index = len(unique_values) // 2\n",
    "        middle_value = unique_values[middle_index]\n",
    "        left_partition = dataframe[dataframe[column] <= middle_value]\n",
    "        right_partition = dataframe[dataframe[column] > middle_value]\n",
    "    else:\n",
    "        # Skip columns that are neither numeric nor categorical\n",
    "        return None, None\n",
    "    \n",
    "    if len(left_partition) >= k and len(right_partition) >= k:\n",
    "        return left_partition, right_partition\n",
    "    else:\n",
    "        # Adjust partitions if lengths are less than k\n",
    "        left_partition = dataframe.iloc[:k]\n",
    "        right_partition = dataframe.iloc[k:]\n",
    "        return left_partition, right_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left partition shape for column 'gender': (490, 9)\n",
      "Right partition shape for column 'gender': (510, 9)\n",
      "Left partition shape for column 'city': (679, 9)\n",
      "Right partition shape for column 'city': (321, 9)\n",
      "Left partition shape for column 'education': (494, 9)\n",
      "Right partition shape for column 'education': (506, 9)\n",
      "Left partition shape for column 'profession': (365, 9)\n",
      "Right partition shape for column 'profession': (635, 9)\n",
      "Left partition shape for column 'age': (504, 9)\n",
      "Right partition shape for column 'age': (496, 9)\n"
     ]
    }
   ],
   "source": [
    "# Esegui lo split per ogni colonna in all_qis\n",
    "for column in all_qis:\n",
    "    left, right = splitter(data, column, k)\n",
    "    if left is not None and right is not None:\n",
    "        print(f\"Left partition shape for column '{column}': {left.shape}\")\n",
    "        print(f\"Right partition shape for column '{column}': {right.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di partizioni: 289\n"
     ]
    }
   ],
   "source": [
    "def recursive_partition(dataset, k, sensitive_data):\n",
    "    \"\"\"Splits the dataset in partitions recursively.\"\"\"\n",
    "    \n",
    "    def axe_to_split(dataframe, sensitive_data):\n",
    "        # Find column with highest cardinality (unique values) to split on\n",
    "        columns_to_exclude = [col for col in dataframe.columns if col in sensitive_data]\n",
    "        max_cardinality_column = dataframe.drop(columns_to_exclude, axis=1).nunique().idxmax()\n",
    "        return max_cardinality_column\n",
    "    \n",
    "    # Base case: if dataset size is smaller than k*2, add it to partitions list\n",
    "    if len(dataset) < k * 2:\n",
    "        dataframe_partitions.append(dataset)\n",
    "    else:\n",
    "        # Split according to column with highest cardinality\n",
    "        axe = axe_to_split(dataset, sensitive_data)\n",
    "        left_partition, right_partition = splitter(dataset, axe, k)\n",
    "        \n",
    "        # Recursively partition left and right partitions\n",
    "        recursive_partition(left_partition, k, sensitive_data)\n",
    "        recursive_partition(right_partition, k, sensitive_data)\n",
    "\n",
    "# Example usage:\n",
    "dataframe_partitions = []\n",
    "k = 3\n",
    "sensitive_data = ['annual_income']  # List of sensitive columns to exclude from splitting\n",
    "data = drop_EI(data, explicit_identifier)\n",
    "recursive_partition(data, k, sensitive_data)\n",
    "\n",
    "# Stampa il numero di partizioni create\n",
    "print(f\"Numero di partizioni: {len(dataframe_partitions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_partition(partition, qis, json_files, statistic):\n",
    "    \"\"\"Generalizza una partizione del dataset sostituendo i valori dei quasi-identificatori con la loro generalizzazione.\n",
    "        Se il quasi-identificatore è numerico, viene generalizzato con il range di valori o la media.\n",
    "        Se il quasi-identificatore è categorico, viene generalizzato con l'antenato comune più basso (LCA).\n",
    "\n",
    "    Args:\n",
    "        partition (DataFrame): Partizione del dataset da generalizzare.\n",
    "        qis (list): Lista dei quasi-identificatori da considerare.\n",
    "        json_files (dict): Dizionario contenente i percorsi ai file JSON per i QI categorici.\n",
    "        statistic (str): Metodo di generalizzazione per i quasi-identificatori numerici ('range' o 'mean').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Partizione del dataset generalizzata.\n",
    "    \"\"\"\n",
    "    for qi in qis:\n",
    "        # Ordina la partizione in base al quasi-identificatore corrente\n",
    "        partition = partition.sort_values(by=qi)\n",
    "        \n",
    "        # Controlla se tutti i valori sono uguali per il quasi-identificatore corrente\n",
    "        if partition[qi].iloc[0] != partition[qi].iloc[-1]:\n",
    "            if qi in numerical_qis:\n",
    "                if statistic == 'range':\n",
    "                    # Se il quasi-identificatore è numerico, generalizza con il range di valori\n",
    "                    min_val = partition[qi].iloc[0]\n",
    "                    max_val = partition[qi].iloc[-1]\n",
    "                    s = f\"[{min_val} - {max_val}]\"\n",
    "                elif statistic == 'mean':\n",
    "                    # Se il quasi-identificatore è numerico e la statistica è 'mean', generalizza con la media\n",
    "                    mean_val = partition[qi].mean()\n",
    "                    s = f\"[{mean_val}]\"\n",
    "                else:\n",
    "                    raise ValueError(\"Statistic must be 'range' or 'mean'\")\n",
    "            else:\n",
    "                # Se il quasi-identificatore è categorico, cerca l'antenato comune più basso (LCA)\n",
    "                unique_values = sorted(set(partition[qi]))\n",
    "                if len(unique_values) == 1:\n",
    "                    lca = unique_values[0]\n",
    "                else:\n",
    "                    lca = unique_values[0]\n",
    "                    for value in unique_values[1:]:\n",
    "                        lca_candidate = find_lowest_common_ancestor(json_files[qi], lca, value, key='name')\n",
    "                        if lca_candidate:\n",
    "                            lca = lca_candidate\n",
    "                        else:\n",
    "                            lca = 'ANY'\n",
    "                            break\n",
    "                        \n",
    "                s = f\"[{lca}]\"\n",
    "            \n",
    "            # Sostituisce i valori del quasi-identificatore con la generalizzazione trovata\n",
    "            partition[qi] = [s] * partition[qi].size\n",
    "    \n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = {\n",
    "    'city': 'generation/cities.json',\n",
    "    'profession': 'generation/jobs.json',\n",
    "    'education': 'generation/educations.json',\n",
    "    'gender': 'generation/genders.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mondrian(database, k, qis, sd, json_files):\n",
    "    anonymized_data = recursive_partition(database, k, sd)\n",
    "    generalized_partitions = []\n",
    "    for i, partition in enumerate(dataframe_partitions):\n",
    "        generalized_partition = generalize_partition(partition, qis, json_files, statistic)\n",
    "        generalized_partitions.append(generalized_partition)\n",
    "    anonymized_data = pd.concat(generalized_partitions)\n",
    "    anonymized_data.to_csv('anonymized.csv', index=False)\n",
    "    print(\"Dati anonimizzati salvati in anonymized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati anonimizzati salvati in anonymized.csv\n"
     ]
    }
   ],
   "source": [
    "anonymized_data = mondrian(data, k, all_qis, sensitive_data, json_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
